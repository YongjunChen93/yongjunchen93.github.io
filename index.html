<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
  <head>
    <meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <link rel="stylesheet" href="jemdoc.css" type="text/css" />

    <style>
      .intro{
        font-size: 18px;
        font-weight: 400;
        letter-spacing: 0.2px;
        margin: 12px 0 18px 0;
      }

      .type-cursor{
        display: inline-block;
        height: 1em;
        margin-left: 3px;
        vertical-align: -0.1em;

        border-left: 3px solid currentColor;  /* ✅ 关键 */
        opacity: 0.8;

        animation: blink 0.9s steps(1) infinite;
      }

      @keyframes blink{
        50% { opacity: 0; }
      }

      /* 打完后让光标慢一点（可选） */
      .type-done .type-cursor{
        animation-duration: 2.5s;
      }
      body {
          font-family: 'Helvetica', sans-serif;
          margin-right: 10%;
          margin-left: 3%;
      }
    </style>

    <title>Yongjun Chen </title>

    <!-- copied this email-hiding script from Silias Boyd-Wickizer
                  https://pdos.csail.mit.edu/~sbw/ -->
    <script type="text/javascript">
      function toggle_abstract(elem) {
        var as = elem.parentNode.parentNode.getElementsByClassName("abstract");
        if (as.length > 0) {
          var a = as[0];
          a.className = "abstract-show"
        }
        else {
          as = elem.parentNode.parentNode.getElementsByClassName("abstract-show");
          var a = as[0];
          a.className = "abstract"
        }
      }
    </script>

    <script type="text/javascript" async
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
    </script>

    <style>
      .abstract { display: none; }
      .abstract-show {
        margin: 10px 0 0 0;
        padding: 7px;
        border: 1px dotted #A09040;
        text-align: justify;
      }
    </style>

    <!-- ✅ NEW: Papers grid layout (border only wraps image) -->
    <style>
    /* ===== Blogs: text-only cards ===== */
      /* ===== Blogs: text-only cards ===== */
    
    
    .blogs-grid{
      display: grid;
      grid-template-columns: repeat(3, minmax(0, 1fr));

      column-gap: 80px;  /* 横向留白，保持高级感 */
      row-gap: 32px;     /* 纵向紧凑，适合阅读 */

      margin-top: 8px;
      margin-bottom: 8px;
      padding: 0;
      list-style: none;
    }

      @media (max-width: 1100px){
        .blogs-grid{ grid-template-columns: repeat(2, minmax(0, 1fr)); }
      }
      @media (max-width: 700px){
        .blogs-grid{ grid-template-columns: 1fr; }
      }

      .blog-card{
        max-width: 560px;
        margin: 0;
        justify-self: start;

        min-height: 240px;
        display: flex;
        flex-direction: column;
      }

      .blog-title{
        margin: 0;
        font-weight: 800;
        line-height: 1.15;
        text-align: left;
      }

      .blog-title a{
        text-decoration: none;
        color: #111;
      }

      .blog-meta{
        display: flex;
        gap: 12px;
        font-size: 13px;
        color: #6b7280;
        margin: 10px 0 14px 0;
      }

      .blog-category,
      .blog-date{
        font-weight: 500;
        white-space: nowrap;
      }

      .blog-excerpt{
        font-size: 16px;
        line-height: 1.7;
        color: #222;
        margin: 0;

        display: -webkit-box;
        -webkit-line-clamp: 4;
        -webkit-box-orient: vertical;
        overflow: hidden;

        margin-top: 6px;
      }

      /* ===== Paper: text + image ===== */
      .papers-grid{
        display: grid;
        grid-template-columns: repeat(3, minmax(0, 1fr));

        /* ✅ 一致的 rhythm：横向呼吸 + 纵向紧凑 */
        column-gap: 80px;   /* 左右间距（保持高级感） */
        row-gap: 32px;      /* 上下间距（统一节奏） */

        margin-top: 8px;
        margin-bottom: 8px;
      }

      @media (max-width: 1100px){
        .papers-grid{ grid-template-columns: repeat(2, minmax(0, 1fr)); }
      }
      @media (max-width: 700px){
        .papers-grid{ grid-template-columns: 1fr; }
      }

      /* 这个外层不加边框：只负责布局 */
      .paper-card{
        background: transparent;
      }

      /* ✅ 边框只包图片 */
      .paper-media{
        border: 1px solid #e7e7e7;
        border-radius: 12px;
        overflow: hidden;
        background: #fff;
        margin-bottom: 10px;  /* 图片边框与文字的间距 */
        max-width: 90%;
        margin-left: 0;
        margin-right: auto;
      }

      .paper-thumb{
        width: 100%;
        height: 160px;          /* 想更小：200px；想更大：240px */
        object-fit: contain;      /* “自然分割/杂志裁切” */
        display: block;
        cursor: zoom-in;
      }

      .paper-title{
        font-size: 16px;
        font-weight: 700;
        margin: 0 0 6px 0;
        line-height: 1.2;
      }

      .paper-authors{
        font-size: 13px;
        font-style: italic;
        margin: 0 0 6px 0;
        line-height: 1.25;
      }

      .paper-venue{
        font-size: 13px;
        color: #6b7280;
        margin-left: 0 0 6px 0;
      }
      .paper-links{
        font-size: 13px;
        line-height: 1.25;
      }

      .paper-links a{
        text-decoration: none;
      }
    </style>
  </head>

  <script>
  document.addEventListener("DOMContentLoaded", () => {
    // Create overlay once
    const overlay = document.createElement("div");
    overlay.className = "lightbox-overlay";
    overlay.innerHTML = `<img alt="">`;
    document.body.appendChild(overlay);

    const overlayImg = overlay.querySelector("img");

    const open = (src, alt) => {
      overlayImg.src = src;
      overlayImg.alt = alt || "";
      overlay.classList.add("is-open");
      document.body.style.overflow = "hidden";
    };

    const close = () => {
      overlay.classList.remove("is-open");
      overlayImg.src = "";
      document.body.style.overflow = "";
    };

    // Click to open (delegation)
    document.addEventListener("click", (e) => {
      const img = e.target.closest("img.zoomable");
      if (!img) return;
      open(img.src, img.alt);
    });

    // Click overlay to close
    overlay.addEventListener("click", close);

    // ESC to close
    document.addEventListener("keydown", (e) => {
      if (e.key === "Escape" && overlay.classList.contains("is-open")) close();
    });
  });
  </script>

  <body>
    <!-- <h1>Yongjun Chen</h1> -->
    <section class="intro">
      <span id="type-prefix"></span>
      <a id="type-link" href="https://www.augmentcode.com/" style="display:none;"><strong>Augment Code</strong></a><span id="type-suffix"></span>
      <span class="type-cursor" aria-hidden="true"></span>
    </section>

    <h2><a id="blogs-link" href="blog/">Blogs & Notes</a></h2>

    <script>
      (function () {
        const a = document.getElementById("blogs-link");
        if (!a) return;
        const isFile = location.protocol === "file:";
        a.href = isFile ? "http://127.0.0.1:4000/blog/" : "blog/";
      })();
    </script>

    <ul id="blog-list" class="blogs-grid">
      <li>Loading…</li>
    </ul>

    <!-- Blogs loader -->
    <script>
    (function () {
      const container = document.getElementById("blog-list");
      if (!container) return;

      const isFile = (location.protocol === "file:");

      // ✅ 线上用相对路径（支持 GitHub Pages repo 子路径）
      const BLOG_BASE = isFile
        ? "http://127.0.0.1:4000/blog"
        : (new URL("./blog", location.href)).toString().replace(/\/$/, "");

      // ✅ cache-busting：防止浏览器执行旧版 posts.js（你现在的核心问题）
      const FEED_JS = BLOG_BASE + "/posts.js?v=" + Date.now();

      const s = document.createElement("script");
      s.src = FEED_JS;
      s.async = true;

      const stripHtml = (html) =>
        (html || "").replace(/<[^>]*>/g, "").replace(/\s+/g, " ").trim();

      const pickCategory = (p) => {
        const cats = p.categories || p.category || [];
        if (Array.isArray(cats) && cats.length) return String(cats[0]);
        if (typeof cats === "string" && cats) return cats;
        return "未分类";
      };

      const pickExcerpt = (p) => {
        const candidates = [p.excerpt, p.summary, p.description, p.content];
        for (const c of candidates) {
          const t = stripHtml(c);
          if (t) return t;
        }
        return "";
      };

      s.onload = () => {
        const posts = window.__BLOG_POSTS__;
        if (!Array.isArray(posts) || posts.length === 0) {
          container.innerHTML = "<li>No posts yet.</li>";
          return;
        }

        const limit = 9;

        container.innerHTML = posts.slice(0, limit).map((p, idx) => {
          const href = BLOG_BASE + (p.url || "");
          const title = p.title || "Untitled";
          const date = p.date || "";
          const category = pickCategory(p);
          const excerpt = pickExcerpt(p);

          // Check if this post should show canvas animation
          if (p.preview_type === 'canvas_animation') {
            return `
              <li>
                <article class="blog-card">
                  <h3 class="blog-title"><a href="${href}">${title}</a></h3>
                  <div class="blog-meta">
                    <span class="blog-category">${category}</span>
                    <span class="blog-date">${date}</span>
                  </div>
                  <canvas id="preview-canvas-${idx}" width="280" height="180" style="display: block; margin: 10px auto; background: #fff;"></canvas>
                </article>
              </li>
            `;
          }

          return `
            <li>
              <article class="blog-card">
                <h3 class="blog-title"><a href="${href}">${title}</a></h3>
                <div class="blog-meta">
                  <span class="blog-category">${category}</span>
                  <span class="blog-date">${date}</span>
                </div>
                <p class="blog-excerpt">${excerpt}</p>
              </article>
            </li>
          `;
        }).join("");

        // Initialize canvas animations after rendering
        posts.slice(0, limit).forEach((p, idx) => {
          if (p.preview_type === 'canvas_animation') {
            initGameOfLifePreview(idx);
          }
        });
      };

      s.onerror = () => {
        container.innerHTML = `<li>Failed to load posts. <a href="${BLOG_BASE}/">Open blog</a></li>`;
      };

      document.head.appendChild(s);
    })();

    // Game of Life preview animation
    function initGameOfLifePreview(idx) {
      const canvas = document.getElementById(`preview-canvas-${idx}`);
      if (!canvas) return;

      const ctx = canvas.getContext('2d');
      const CELL_SIZE = 4;
      const COLS = Math.floor(canvas.width / CELL_SIZE);
      const ROWS = Math.floor(canvas.height / CELL_SIZE);

      let grid = Array(ROWS).fill(null).map(() => Array(COLS).fill(0));

      // Gosper Gun pattern
      const pattern = [
        [5,1],[5,2],[6,1],[6,2],[5,11],[6,11],[7,11],[4,12],[3,13],[3,14],[8,12],[9,13],[9,14],
        [6,15],[4,16],[5,17],[6,17],[7,17],[6,18],[8,16],[3,21],[4,21],[5,21],[3,22],[4,22],
        [5,22],[2,23],[6,23],[1,25],[2,25],[6,25],[7,25],[3,35],[4,35],[3,36],[4,36]
      ];

      // Place pattern
      const offsetX = 5;
      const offsetY = 5;
      pattern.forEach(([r, c]) => {
        if (r + offsetY < ROWS && c + offsetX < COLS) {
          grid[r + offsetY][c + offsetX] = 1;
        }
      });

      function countNeighbors(r, c) {
        let count = 0;
        for (let dr = -1; dr <= 1; dr++) {
          for (let dc = -1; dc <= 1; dc++) {
            if (dr === 0 && dc === 0) continue;
            const nr = (r + dr + ROWS) % ROWS;
            const nc = (c + dc + COLS) % COLS;
            count += grid[nr][nc];
          }
        }
        return count;
      }

      function update() {
        const newGrid = Array(ROWS).fill(null).map(() => Array(COLS).fill(0));
        for (let r = 0; r < ROWS; r++) {
          for (let c = 0; c < COLS; c++) {
            const neighbors = countNeighbors(r, c);
            if (grid[r][c] === 1) {
              newGrid[r][c] = (neighbors === 2 || neighbors === 3) ? 1 : 0;
            } else {
              newGrid[r][c] = (neighbors === 3) ? 1 : 0;
            }
          }
        }
        grid = newGrid;
      }

      function draw() {
        ctx.fillStyle = '#fff';
        ctx.fillRect(0, 0, canvas.width, canvas.height);
        ctx.fillStyle = '#000';
        for (let r = 0; r < ROWS; r++) {
          for (let c = 0; c < COLS; c++) {
            if (grid[r][c] === 1) {
              ctx.fillRect(c * CELL_SIZE, r * CELL_SIZE, CELL_SIZE - 1, CELL_SIZE - 1);
            }
          }
        }
      }

      function animate() {
        update();
        draw();
        setTimeout(animate, 100);
      }

      draw();
      animate();
    }
    </script>

    <h2>Selected Papers</h2>

    <!-- ✅ NEW Papers Layout -->
    <div class="papers-grid">

      <!-- Paper 1 -->
      <div class="paper-card">
        <!-- <div class="paper-media">
          <img
            src="images/icl.png"
            alt="Intent Contrastive Learning for Sequential Recommendation"
            class="paper-thumb zoomable"
          />
        </div> -->

        <div class="paper-title">Intent Contrastive Learning for Sequential Recommendation</div>
        <div class="paper-authors">Yongjun Chen, Zhiwei Liu, Jia Li, Julian McAuley, Caiming Xiong</div>
        <div class="paper-links">
          [<a href="javascript:void(0)" onclick="toggle_abstract(this)">Abstract</a>]
          [<a href="https://arxiv.org/pdf/2202.02519.pdf">Paper</a>]
          [<a href="https://github.com/salesforce/ICLRec">Code</a>]
        </div>
        <div class="paper-venue">WWW 2022</div>

        <div class="abstract" style="line-height:20px">
          Users’ interactions with items are driven by various intents (e.g., preparing for holiday gifts, shopping for fishing equipment, etc.). However, users’ underlying intents are often unobserved/latent,making it challenging to leverage such a latent intent factor for Sequential recommendation(SR). To investigate the benefits of latent intent and leverage it effectively for recommendation, we proposeIntentContrastiveLearning(ICL), a general learning paradigm that leverages a latent intent variable into SR. The core idea is to learn users’ intent distribution functions from unlabeled user behavior sequences and optimize SR models with contrastive self-supervised learning (SSL) by considering the learnt intents to improve recommendation. Specifically, we introduce a latent variable to represent users’ intents and learn the distribution function of the latent variable via clustering. We propose to leverage the learnt intents intoSR models via contrastive SSL, which maximizes the agreement between a view of sequence and its corresponding intent. The training is alternated between intent representation learning and the SR model optimization steps within the generalized expectation-maximization (EM) framework. Fusing user intent information intoSR also improves model robustness. Experiments conducted on four real-world datasets demonstrate the superiority of the proposed learning paradigm, which improves performance, and robustness against data sparsity and noisy interaction issues. Case studies onSports and Yelp further verify the effectiveness of ICL.
        </div>
      </div>

      <!-- Paper 2 -->
      <div class="paper-card">

        <div class="paper-title">Voxel Deconvolutional Networks for 3D Image Labeling</div>
        <div class="paper-authors">Yongjun Chen, Hongyang Gao, Lei Cai, Min Shi, Dinggang Shen and Shuiwang Ji</div>
        <div class="paper-links">
          [<a href="javascript:void(0)" onclick="toggle_abstract(this)">Abstract</a>]
          [<a href="http://delivery.acm.org/10.1145/3220000/3219974/p1226-chen.pdf?ip=69.166.46.137&id=3219974&acc=OPENTOC&key=B63ACEF81C6334F5%2E3B1D11B7501B70D8%2E4D4702B0C3E38B35%2E054E54E275136550&__acm__=1535680435_e69225cf8c7ed216e2e0a11ee85f4452">Paper</a>]
          [<a href="https://github.com/divelab/VoxelDCN">Code</a>]
          [<a href="https://www.eecs.wsu.edu/~ychen3/kdd_slides_vdn.pdf">Slides</a>]
          [<a href="https://www.eecs.wsu.edu/~ychen3/kdd_poster_vdn.pdf">Poster</a>]
        </div>
        <div class="paper-venue">KDD 2018</div>

        <div class="abstract" style="line-height:20px">
          Deep learning methods have shown great success in pixel-wise
          prediction tasks. One of the most popular methods employs an
          encoder-decoder network in which deconvolutional layers are used
          for up-sampling feature maps. However, a key limitation of the
          deconvolutional layer is that it suffers from the checkerboard artifact
          problem, which harms the prediction accuracy. This is caused
          by the independency among adjacent pixels on the output feature
          maps. Previous work only solved the checkerboard artifact issue of
          deconvolutional layers in the 2D space. Since the number of intermediate
          feature maps needed to generate a deconvolutional layer
          grows exponentially with dimensionality, it is more challenging to
          solve this issue in higher dimensions. In this work, we propose the
          voxel deconvolutional layer (VoxelDCL) to solve the checkerboard
          artifact problem of deconvolutional layers in 3D space. We also
          provide an efficient approach to implement VoxelDCL. To demonstrate
          the effectiveness of VoxelDCL, we build four variations of
          voxel deconvolutional networks (VoxelDCN) based on the U-Net
          architecture with VoxelDCL. We apply our networks to address
          volumetric brain images labeling tasks using the ADNI and LONI
          LPBA40 datasets. The experimental results show that the proposed
          iVoxelDCNa achieves improved performance in all experiments.
          It reaches 83.34% in terms of dice ratio on the ADNI dataset and
          79.12% on the LONI LPBA40 dataset, which increases 1.39% and
          2.21% respectively compared with the baseline. In addition, all the
          variations of VoxelDCN we proposed outperform the baseline methods
          on the above datasets, which demonstrates the effectiveness of
          our methods.
        </div>
      </div>

      <!-- Paper 3 -->
      <div class="paper-card">
        <div class="paper-title">Dense Transformer Networks</div>
        <div class="paper-authors">Jun Li, Yongjun Chen, Lei Cai, Ian Davidson, and Shuiwang Ji</div>
        <div class="paper-links">
          [<a href="javascript:void(0)" onclick="toggle_abstract(this)">Abstract</a>]
          [<a href="https://arxiv.org/abs/1705.08881">Paper</a>]
          [<a href="https://github.com/divelab/dtn">Code</a>]
        </div>
        <div class="paper-venue">IJCAI 2019</div>

        <div class="abstract" style="line-height:20px">
          The key idea of current deep learning methods for dense prediction is to apply a model on a regular patch centered on each pixel to make pixel-wise predictions. These methods are limited in the sense that the patches are determined by network architecture instead of learned from data. In this work, we propose the dense transformer networks, which can learn the shapes and sizes of patches from data. The dense transformer networks employ an encoder-decoder architecture, and a pair of dense transformer modules are inserted into each of the encoder and decoder paths. The novelty of this work is that we provide technical solutions for learning the shapes and sizes of patches from data and efficiently restoring the spatial correspondence required for dense prediction. The proposed dense transformer modules are differentiable, thus the entire network can be trained. We apply the proposed networks on natural and biological image segmentation tasks and show superior performance is achieved in comparison to baseline methods.
        </div>
      </div>

    </div>
    <!-- ✅ END Papers -->
  
  <section class="about-me not-prose text-sm md:text-base text-gray-600 leading-relaxed">
    <h2>About Me</h2>
      <table class="imgtable about-muted"><tr><td>
        <img style="" src="images/selfie2.jpeg" alt="photography" width="360px" height="360px"/>&nbsp;</td>
        <td align="left">
          <p><font> I’m currently researching and building AI coding intelligence at <a href="https://augmentcode.com/"><strong>Augment Code</strong></a>.</p>
          
          <p>I was a senior research engineer at <a href="https://www.apple.com/"><strong>Apple</strong></a> (2022-2025), where I worked on <a href="https://www.apple.com/apple-intelligence/"><strong>Apple Intelligence</strong></a> initiatives end-to-end.
          Efforts spanning benchmark training infra in early days, production data synthesis and mixture design, post-training and model distillation experimentation for both large and small models, and building ReAct style long-horizon reinforcement learning for aligning agentic models.
          Influenced AI feature launches including <a href="https://support.apple.com/guide/iphone/find-the-right-words-with-writing-tools-iph6f08da1d2/ios"><strong>Writing Tools</strong></a> and <a href="https://support.apple.com/guide/iphone/use-apple-intelligence-in-messages-iph64709c5c3/ios"><strong>Reply Suggestions</strong></a>. Before Apple, I was a senior research engineer at <a href="https://www.salesforceairesearch.com/"><strong>Salesforce AI Research</strong></a> 
          (2019-2022) and worked on both research and applications of sequential recommender systems, and AutoML. A 4th-place finish is awarded in the
          <a href="https://algo.browser.qq.com/#en">CIKM 2021 AutoML competition</a>.</p>
          
          I received an M.S. in Computer Science from
          <a href="https://wsu.edu/">Washington State University</a>, with a thesis on
          <a href="https://rex.libraries.wsu.edu/esploro/outputs/graduate/Advanced-Deep-Learning-Methods-for-Image/99900525175901842">
          <strong>advanced deep learning methods for pixel-wise prediction</strong></a>, and a B.S. in Statistics from <a href="http://english.hust.edu.cn/">Huazhong University of Science and Technology</a>.
        </td></tr>
      </table>
    </section>
    <font><strong>Contact:</strong></font> <a style="color:black;" href="mailto:yongjunchen1995@gmail.com">yongjunchen1995@gmail.com</a>
    <script>
      const isLocal = location.protocol === "file:" || location.hostname === "localhost" || location.hostname === "127.0.0.1";
      const blogLink = isLocal
        ? "http://127.0.0.1:4000/blog/"
        : "blog/";
      document.write(`<a href="${blogLink}">[Blog]</a>`);
    </script>
    <a href="https://www.linkedin.com/in/YongjunChen/">[Linkedin]</a>
    <a href="https://scholar.google.com/citations?user=XixFvLIAAAAJ&hl=en">[Google Scholar]</a>
    <a href="https://github.com/YongjunChen93">[Github]</a>
    <!-- <h3>DM/ML/AI Conference Reviewer</h3>
        <ul>
        <li><p>2025: COLING; 2024: TKDE, ACL, EMNLP; 2023: ACL; 2022: UAI, EMNLP, AAAI, ACML, TOIS; 2021: NAACL</p></li>
        <li><p>2020 and before: KDD, CIKM, NeurIPS, ICDM, TKDE</p></li>
        </ul> -->
  </body>

  <script>
  (function () {
    const prefixEl = document.getElementById("type-prefix");
    const linkEl   = document.getElementById("type-link");
    const suffixEl = document.getElementById("type-suffix");
    if (!prefixEl || !linkEl || !suffixEl) return;

    const PREFIX = "Building AI coding intelligence at ";
    const LINK_TEXT = "Augment Code";
    const SUFFIX = ".";

    const speed = 22;     // 每个字符间隔 ms，越小越快
    const jitter = 18;    // 抖动，让它更像人敲（可选）

    let i = 0;

    function stepPrefix() {
      if (i <= PREFIX.length) {
        prefixEl.textContent = PREFIX.slice(0, i++);
        setTimeout(stepPrefix, speed + Math.random() * jitter);
      } else {
        // 开始“打出”链接：先显示链接，再逐字补全其文本
        linkEl.style.display = "inline";
        linkEl.textContent = "";
        i = 0;
        stepLink();
      }
    }

    function stepLink() {
      if (i <= LINK_TEXT.length) {
        linkEl.textContent = LINK_TEXT.slice(0, i++);
        setTimeout(stepLink, speed + Math.random() * jitter);
      } else {
        // 最后补句号
        suffixEl.textContent = SUFFIX;
        document.body.classList.add("type-done");
      }
    }

    // 可选：尊重“减少动效”
    const reduce = window.matchMedia && window.matchMedia("(prefers-reduced-motion: reduce)").matches;
    if (reduce) {
      prefixEl.textContent = PREFIX;
      linkEl.style.display = "inline";
      linkEl.textContent = LINK_TEXT;
      suffixEl.textContent = SUFFIX;
      document.body.classList.add("type-done");
      return;
    }

    stepPrefix();
  })();
  </script>
</html>
